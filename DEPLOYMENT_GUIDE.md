# è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹æœåŠ¡ç«¯éƒ¨ç½²æŒ‡å—

## ç›®å½•
1. [ç¯å¢ƒè¦æ±‚](#ç¯å¢ƒè¦æ±‚)
2. [é¡¹ç›®ç»“æ„](#é¡¹ç›®ç»“æ„)
3. [å®‰è£…æ­¥éª¤](#å®‰è£…æ­¥éª¤)
4. [APIæœåŠ¡ä»£ç ](#apiæœåŠ¡ä»£ç )
5. [Dockeréƒ¨ç½²](#dockeréƒ¨ç½²)
6. [æµ‹è¯•API](#æµ‹è¯•api)
7. [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ç¯å¢ƒè¦æ±‚

### ç¡¬ä»¶è¦æ±‚
- **GPU**: NVIDIA GPU (æ¨èRTX 3060æˆ–æ›´é«˜)
- **æ˜¾å­˜**: è‡³å°‘4GB (æ¨¡å‹1GB + æ¨ç†å¼€é”€)
- **å†…å­˜**: è‡³å°‘8GB RAM
- **å­˜å‚¨**: è‡³å°‘10GBå¯ç”¨ç©ºé—´

### è½¯ä»¶è¦æ±‚
- **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ / CentOS 7+ / Windows 10+ (æ¨èLinux)
- **Python**: 3.8 - 3.11
- **CUDA**: 11.8 æˆ– 12.1 (ä¸PyTorchç‰ˆæœ¬åŒ¹é…)
- **cuDNN**: å¯¹åº”CUDAç‰ˆæœ¬çš„cuDNN

---

## é¡¹ç›®ç»“æ„

åˆ›å»ºä»¥ä¸‹ç›®å½•ç»“æ„ï¼š

```
speech-to-speech-backend/
â”œâ”€â”€ app.py                 # Flask/FastAPIä¸»ç¨‹åº
â”œâ”€â”€ model_loader.py        # æ¨¡å‹åŠ è½½å’Œæ¨ç†
â”œâ”€â”€ requirements.txt       # Pythonä¾èµ–
â”œâ”€â”€ Dockerfile            # Dockeré…ç½®
â”œâ”€â”€ docker-compose.yml    # Docker Composeé…ç½®
â”œâ”€â”€ models/               # æ¨¡å‹æ–‡ä»¶ç›®å½•
â”‚   â””â”€â”€ your_model.pt     # æ‚¨çš„PyTorchæ¨¡å‹
â”œâ”€â”€ uploads/              # ä¸´æ—¶ä¸Šä¼ æ–‡ä»¶
â””â”€â”€ outputs/              # å¤„ç†åçš„éŸ³é¢‘è¾“å‡º
```

---

## å®‰è£…æ­¥éª¤

### æ–¹æ³•1: ç›´æ¥å®‰è£… (æ¨èç”¨äºå¼€å‘)

#### 1. å®‰è£…CUDAå’ŒcuDNN

```bash
# Ubuntuç¤ºä¾‹ - å®‰è£…CUDA 11.8
wget https://developer.download.nvidia.com/compute/cuda/11.8.0/local_installers/cuda_11.8.0_520.61.05_linux.run
sudo sh cuda_11.8.0_520.61.05_linux.run

# æ·»åŠ åˆ°ç¯å¢ƒå˜é‡
echo 'export PATH=/usr/local/cuda-11.8/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/local/cuda-11.8/lib64:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc

# éªŒè¯å®‰è£…
nvcc --version
nvidia-smi
```

#### 2. åˆ›å»ºPythonè™šæ‹Ÿç¯å¢ƒ

```bash
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir speech-to-speech-backend
cd speech-to-speech-backend

# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate  # Linux/Mac
# venv\Scripts\activate   # Windows

# å‡çº§pip
pip install --upgrade pip
```

#### 3. å®‰è£…PyTorch (GPUç‰ˆæœ¬)

```bash
# CUDA 11.8
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# CUDA 12.1
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121

# éªŒè¯GPUå¯ç”¨
python -c "import torch; print(f'CUDA available: {torch.cuda.is_available()}'); print(f'GPU: {torch.cuda.get_device_name(0)}')"
```

#### 4. å®‰è£…å…¶ä»–ä¾èµ–

åˆ›å»º `requirements.txt`:

```txt
flask==3.0.0
flask-cors==4.0.0
torch>=2.0.0
torchaudio>=2.0.0
numpy>=1.24.0
scipy>=1.10.0
librosa>=0.10.0
soundfile>=0.12.0
```

å®‰è£…ï¼š

```bash
pip install -r requirements.txt
```

---

## APIæœåŠ¡ä»£ç 

### 1. åˆ›å»º `model_loader.py`

```python
import torch
import torchaudio
import numpy as np
from pathlib import Path

class SpeechToSpeechModel:
    def __init__(self, model_path: str, device: str = 'cuda'):
        """
        åˆå§‹åŒ–è¯­éŸ³åˆ°è¯­éŸ³æ¨¡å‹
        
        Args:
            model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„ (.ptæ–‡ä»¶)
            device: è¿è¡Œè®¾å¤‡ ('cuda' æˆ– 'cpu')
        """
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½æ¨¡å‹
        print(f"åŠ è½½æ¨¡å‹: {model_path}")
        self.model = torch.load(model_path, map_location=self.device)
        self.model.eval()
        print("æ¨¡å‹åŠ è½½å®Œæˆ")
        
    def preprocess_audio(self, audio_path: str, target_sr: int = 16000):
        """
        é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_path: è¾“å…¥éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            target_sr: ç›®æ ‡é‡‡æ ·ç‡
            
        Returns:
            å¤„ç†åçš„éŸ³é¢‘å¼ é‡
        """
        # åŠ è½½éŸ³é¢‘
        waveform, sample_rate = torchaudio.load(audio_path)
        
        # é‡é‡‡æ ·
        if sample_rate != target_sr:
            resampler = torchaudio.transforms.Resample(sample_rate, target_sr)
            waveform = resampler(waveform)
        
        # è½¬ä¸ºå•å£°é“
        if waveform.shape[0] > 1:
            waveform = torch.mean(waveform, dim=0, keepdim=True)
        
        # ç§»åˆ°GPU
        waveform = waveform.to(self.device)
        
        return waveform
    
    def postprocess_audio(self, output_tensor, output_path: str, sample_rate: int = 16000):
        """
        åå¤„ç†æ¨¡å‹è¾“å‡º
        
        Args:
            output_tensor: æ¨¡å‹è¾“å‡ºå¼ é‡
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            sample_rate: é‡‡æ ·ç‡
        """
        # ç§»åˆ°CPU
        output_tensor = output_tensor.cpu()
        
        # ä¿å­˜éŸ³é¢‘
        torchaudio.save(output_path, output_tensor, sample_rate)
        
    @torch.no_grad()
    def process(self, input_audio_path: str, output_audio_path: str):
        """
        å¤„ç†éŸ³é¢‘æ–‡ä»¶
        
        Args:
            input_audio_path: è¾“å…¥éŸ³é¢‘è·¯å¾„
            output_audio_path: è¾“å‡ºéŸ³é¢‘è·¯å¾„
        """
        # é¢„å¤„ç†
        input_tensor = self.preprocess_audio(input_audio_path)
        
        # æ¨ç†
        # æ³¨æ„: è¿™é‡Œéœ€è¦æ ¹æ®æ‚¨çš„æ¨¡å‹å®é™…è¾“å…¥è¾“å‡ºæ ¼å¼è¿›è¡Œè°ƒæ•´
        output_tensor = self.model(input_tensor)
        
        # åå¤„ç†
        self.postprocess_audio(output_tensor, output_audio_path)
        
        return output_audio_path


# ç¤ºä¾‹: å¦‚æœæ‚¨çš„æ¨¡å‹æœ‰ç‰¹æ®Šçš„æ¨ç†æ–¹å¼ï¼Œè¯·ä¿®æ”¹è¿™ä¸ªç±»
class CustomSpeechModel(SpeechToSpeechModel):
    """
    è‡ªå®šä¹‰æ¨¡å‹ç±» - æ ¹æ®æ‚¨çš„æ¨¡å‹ç‰¹ç‚¹ä¿®æ”¹
    """
    
    @torch.no_grad()
    def process(self, input_audio_path: str, output_audio_path: str):
        """
        è‡ªå®šä¹‰æ¨ç†æµç¨‹
        """
        # 1. åŠ è½½å’Œé¢„å¤„ç†
        waveform, sr = torchaudio.load(input_audio_path)
        waveform = waveform.to(self.device)
        
        # 2. æ¨¡å‹æ¨ç† - æ ¹æ®æ‚¨çš„æ¨¡å‹è°ƒæ•´
        # ä¾‹å¦‚: å¦‚æœæ¨¡å‹éœ€è¦ç‰¹å®šçš„è¾“å…¥æ ¼å¼
        # input_features = self.extract_features(waveform)
        # output = self.model(input_features)
        
        output = self.model(waveform)
        
        # 3. ä¿å­˜è¾“å‡º
        output = output.cpu()
        torchaudio.save(output_audio_path, output, sr)
        
        return output_audio_path
```

### 2. åˆ›å»º `app.py` (Flaskç‰ˆæœ¬)

```python
from flask import Flask, request, send_file, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
import os
import uuid
from pathlib import Path
from model_loader import SpeechToSpeechModel

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# é…ç½®
UPLOAD_FOLDER = 'uploads'
OUTPUT_FOLDER = 'outputs'
MODEL_PATH = 'models/your_model.pt'  # ä¿®æ”¹ä¸ºæ‚¨çš„æ¨¡å‹è·¯å¾„
ALLOWED_EXTENSIONS = {'wav', 'mp3', 'flac', 'ogg'}

# åˆ›å»ºå¿…è¦çš„ç›®å½•
Path(UPLOAD_FOLDER).mkdir(exist_ok=True)
Path(OUTPUT_FOLDER).mkdir(exist_ok=True)

# åŠ è½½æ¨¡å‹ (å¯åŠ¨æ—¶åŠ è½½ä¸€æ¬¡)
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = SpeechToSpeechModel(MODEL_PATH, device='cuda')
print("æ¨¡å‹åŠ è½½å®Œæˆï¼ŒAPIæœåŠ¡å·²å¯åŠ¨")

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'gpu_available': torch.cuda.is_available()
    })

@app.route('/api/process', methods=['POST'])
def process_audio():
    """
    å¤„ç†éŸ³é¢‘çš„ä¸»æ¥å£
    
    è¯·æ±‚: multipart/form-data
        - audio: éŸ³é¢‘æ–‡ä»¶
    
    å“åº”: å¤„ç†åçš„éŸ³é¢‘æ–‡ä»¶
    """
    try:
        # æ£€æŸ¥æ–‡ä»¶
        if 'audio' not in request.files:
            return jsonify({'error': 'æ²¡æœ‰ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶'}), 400
        
        file = request.files['audio']
        if file.filename == '':
            return jsonify({'error': 'æ–‡ä»¶åä¸ºç©º'}), 400
        
        if not allowed_file(file.filename):
            return jsonify({'error': 'ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼'}), 400
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        file_id = str(uuid.uuid4())
        input_filename = secure_filename(f"{file_id}_input.wav")
        output_filename = f"{file_id}_output.wav"
        
        input_path = os.path.join(UPLOAD_FOLDER, input_filename)
        output_path = os.path.join(OUTPUT_FOLDER, output_filename)
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        file.save(input_path)
        print(f"æ¥æ”¶åˆ°éŸ³é¢‘æ–‡ä»¶: {input_path}")
        
        # å¤„ç†éŸ³é¢‘
        print("å¼€å§‹å¤„ç†...")
        model.process(input_path, output_path)
        print(f"å¤„ç†å®Œæˆ: {output_path}")
        
        # è¿”å›å¤„ç†åçš„æ–‡ä»¶
        response = send_file(
            output_path,
            mimetype='audio/wav',
            as_attachment=True,
            download_name='processed_audio.wav'
        )
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ (å¯é€‰)
        # os.remove(input_path)
        # os.remove(output_path)
        
        return response
        
    except Exception as e:
        print(f"å¤„ç†é”™è¯¯: {str(e)}")
        return jsonify({'error': str(e)}), 500

@app.route('/api/info', methods=['GET'])
def model_info():
    """è·å–æ¨¡å‹ä¿¡æ¯"""
    import torch
    return jsonify({
        'model_path': MODEL_PATH,
        'device': str(model.device),
        'cuda_available': torch.cuda.is_available(),
        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None,
        'gpu_memory': f"{torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB" if torch.cuda.is_available() else None
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=8000, debug=False)
```

### 3. åˆ›å»º `app.py` (FastAPIç‰ˆæœ¬ - æ¨è)

```python
from fastapi import FastAPI, File, UploadFile, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
import uvicorn
import os
import uuid
from pathlib import Path
from model_loader import SpeechToSpeechModel
import torch

app = FastAPI(title="Speech-to-Speech API")

# CORSé…ç½®
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# é…ç½®
UPLOAD_FOLDER = 'uploads'
OUTPUT_FOLDER = 'outputs'
MODEL_PATH = 'models/your_model.pt'

Path(UPLOAD_FOLDER).mkdir(exist_ok=True)
Path(OUTPUT_FOLDER).mkdir(exist_ok=True)

# åŠ è½½æ¨¡å‹
print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
model = SpeechToSpeechModel(MODEL_PATH, device='cuda')
print("æ¨¡å‹åŠ è½½å®Œæˆ")

@app.get("/health")
async def health_check():
    return {
        'status': 'healthy',
        'model_loaded': model is not None,
        'gpu_available': torch.cuda.is_available()
    }

@app.post("/api/process")
async def process_audio(audio: UploadFile = File(...)):
    try:
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        file_id = str(uuid.uuid4())
        input_path = os.path.join(UPLOAD_FOLDER, f"{file_id}_input.wav")
        output_path = os.path.join(OUTPUT_FOLDER, f"{file_id}_output.wav")
        
        # ä¿å­˜ä¸Šä¼ æ–‡ä»¶
        with open(input_path, "wb") as f:
            content = await audio.read()
            f.write(content)
        
        print(f"æ¥æ”¶åˆ°éŸ³é¢‘: {input_path}")
        
        # å¤„ç†
        model.process(input_path, output_path)
        print(f"å¤„ç†å®Œæˆ: {output_path}")
        
        # è¿”å›æ–‡ä»¶
        return FileResponse(
            output_path,
            media_type="audio/wav",
            filename="processed_audio.wav"
        )
        
    except Exception as e:
        print(f"é”™è¯¯: {str(e)}")
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/api/info")
async def model_info():
    return {
        'model_path': MODEL_PATH,
        'device': str(model.device),
        'cuda_available': torch.cuda.is_available(),
        'gpu_name': torch.cuda.get_device_name(0) if torch.cuda.is_available() else None
    }

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8000)
```

---

## Dockeréƒ¨ç½²

### 1. åˆ›å»º `Dockerfile`

```dockerfile
FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…Pythonå’Œç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    python3.10 \
    python3-pip \
    libsndfile1 \
    ffmpeg \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£…Pythonä¾èµ–
RUN pip3 install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY . .

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p uploads outputs models

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python3", "app.py"]
```

### 2. åˆ›å»º `docker-compose.yml`

```yaml
version: '3.8'

services:
  speech-api:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./models:/app/models
      - ./uploads:/app/uploads
      - ./outputs:/app/outputs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    environment:
      - CUDA_VISIBLE_DEVICES=0
    restart: unless-stopped
```

### 3. æ„å»ºå’Œè¿è¡Œ

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# åœæ­¢æœåŠ¡
docker-compose down
```

---

## æµ‹è¯•API

### ä½¿ç”¨curlæµ‹è¯•

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# è·å–æ¨¡å‹ä¿¡æ¯
curl http://localhost:8000/api/info

# å¤„ç†éŸ³é¢‘
curl -X POST http://localhost:8000/api/process \
  -F "audio=@test_audio.wav" \
  -o output.wav
```

### Pythonæµ‹è¯•è„šæœ¬

åˆ›å»º `test_api.py`:

```python
import requests

API_URL = "http://localhost:8000"

def test_health():
    response = requests.get(f"{API_URL}/health")
    print("å¥åº·æ£€æŸ¥:", response.json())

def test_process(audio_file):
    with open(audio_file, 'rb') as f:
        files = {'audio': f}
        response = requests.post(f"{API_URL}/api/process", files=files)
        
        if response.status_code == 200:
            with open('output.wav', 'wb') as out:
                out.write(response.content)
            print("å¤„ç†æˆåŠŸï¼Œè¾“å‡ºä¿å­˜ä¸º output.wav")
        else:
            print(f"é”™è¯¯: {response.status_code}")
            print(response.json())

if __name__ == "__main__":
    test_health()
    test_process("test_audio.wav")
```

---

## å¸¸è§é—®é¢˜

### 1. CUDA Out of Memory

**é—®é¢˜**: GPUæ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```python
# åœ¨model_loader.pyä¸­æ·»åŠ 
torch.cuda.empty_cache()

# æˆ–ä½¿ç”¨æ··åˆç²¾åº¦
with torch.cuda.amp.autocast():
    output = model(input_tensor)
```

### 2. æ¨¡å‹åŠ è½½å¤±è´¥

**é—®é¢˜**: æ— æ³•åŠ è½½.ptæ–‡ä»¶

**è§£å†³æ–¹æ¡ˆ**:
```python
# å°è¯•ä¸åŒçš„åŠ è½½æ–¹å¼
model = torch.load(model_path, map_location=device)
# æˆ–
checkpoint = torch.load(model_path)
model.load_state_dict(checkpoint['model_state_dict'])
```

### 3. éŸ³é¢‘æ ¼å¼é—®é¢˜

**é—®é¢˜**: ä¸æ”¯æŒæŸäº›éŸ³é¢‘æ ¼å¼

**è§£å†³æ–¹æ¡ˆ**:
```bash
# å®‰è£…ffmpeg
sudo apt-get install ffmpeg

# æˆ–ä½¿ç”¨pydubè½¬æ¢
pip install pydub
```

### 4. ç«¯å£è¢«å ç”¨

```bash
# æŸ¥æ‰¾å ç”¨ç«¯å£çš„è¿›ç¨‹
lsof -i :8000

# æ€æ­»è¿›ç¨‹
kill -9 <PID>

# æˆ–æ›´æ”¹ç«¯å£
python app.py --port 8001
```

### 5. æ€§èƒ½ä¼˜åŒ–

```python
# ä½¿ç”¨TorchScriptä¼˜åŒ–
model = torch.jit.script(model)

# å¯ç”¨cudnn benchmark
torch.backends.cudnn.benchmark = True

# æ‰¹å¤„ç†æ¨ç†
def process_batch(audio_files):
    batch = torch.stack([preprocess(f) for f in audio_files])
    with torch.no_grad():
        outputs = model(batch)
    return outputs
```

---

## ç”Ÿäº§ç¯å¢ƒå»ºè®®

1. **ä½¿ç”¨HTTPS**: é…ç½®SSLè¯ä¹¦
2. **æ·»åŠ è®¤è¯**: å®ç°APIå¯†é’¥æˆ–JWTè®¤è¯
3. **é™æµ**: ä½¿ç”¨Rediså®ç°è¯·æ±‚é™æµ
4. **æ—¥å¿—**: é…ç½®å®Œå–„çš„æ—¥å¿—ç³»ç»Ÿ
5. **ç›‘æ§**: ä½¿ç”¨Prometheus + Grafanaç›‘æ§
6. **è´Ÿè½½å‡è¡¡**: ä½¿ç”¨Nginxåšåå‘ä»£ç†

---

## è”ç³»æ”¯æŒ

å¦‚æœ‰é—®é¢˜ï¼Œè¯·å‚è€ƒ:
- PyTorchæ–‡æ¡£: https://pytorch.org/docs/
- Flaskæ–‡æ¡£: https://flask.palletsprojects.com/
- FastAPIæ–‡æ¡£: https://fastapi.tiangolo.com/

ç¥éƒ¨ç½²é¡ºåˆ©ï¼ğŸš€